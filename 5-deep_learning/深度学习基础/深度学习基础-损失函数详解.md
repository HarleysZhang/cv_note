- [ä¸€ï¼ŒæŸå¤±å‡½æ•°æ¦‚è¿°](#ä¸€æŸå¤±å‡½æ•°æ¦‚è¿°)
- [äºŒï¼Œäº¤å‰ç†µå‡½æ•°-åˆ†ç±»æŸå¤±](#äºŒäº¤å‰ç†µå‡½æ•°-åˆ†ç±»æŸå¤±)
  - [2.1ï¼Œäº¤å‰ç†µï¼ˆCross-Entropyï¼‰çš„ç”±æ¥](#21äº¤å‰ç†µcross-entropyçš„ç”±æ¥)
    - [2.1.1ï¼Œç†µã€ç›¸å¯¹ç†µä»¥åŠäº¤å‰ç†µæ€»ç»“](#211ç†µç›¸å¯¹ç†µä»¥åŠäº¤å‰ç†µæ€»ç»“)
  - [2.2ï¼ŒäºŒåˆ†ç±»é—®é¢˜çš„äº¤å‰ç†µ](#22äºŒåˆ†ç±»é—®é¢˜çš„äº¤å‰ç†µ)
  - [2.3ï¼Œå¤šåˆ†ç±»é—®é¢˜çš„äº¤å‰ç†µ](#23å¤šåˆ†ç±»é—®é¢˜çš„äº¤å‰ç†µ)
  - [2.4ï¼ŒPyTorch ä¸­çš„ Cross Entropy](#24pytorch-ä¸­çš„-cross-entropy)
    - [2.4.1ï¼ŒSoftmax å¤šåˆ†ç±»å‡½æ•°](#241softmax-å¤šåˆ†ç±»å‡½æ•°)
  - [2.5ï¼Œä¸ºä»€ä¹ˆä¸èƒ½ä½¿ç”¨å‡æ–¹å·®åšä¸ºåˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°ï¼Ÿ](#25ä¸ºä»€ä¹ˆä¸èƒ½ä½¿ç”¨å‡æ–¹å·®åšä¸ºåˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°)
- [ä¸‰ï¼Œå›å½’æŸå¤±](#ä¸‰å›å½’æŸå¤±)
  - [3.1ï¼ŒMAE æŸå¤±](#31mae-æŸå¤±)
  - [3.2ï¼ŒMSE æŸå¤±](#32mse-æŸå¤±)
  - [3.3ï¼Œ`Huber` æŸå¤±](#33huber-æŸå¤±)
  - [3.4ï¼Œä»£ç å®ç°](#34ä»£ç å®ç°)
- [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

## ä¸€ï¼ŒæŸå¤±å‡½æ•°æ¦‚è¿°

å¤§å¤šæ•°æ·±åº¦å­¦ä¹ ç®—æ³•éƒ½ä¼šæ¶‰åŠæŸç§å½¢å¼çš„ä¼˜åŒ–ï¼Œ**æ‰€è°“ä¼˜åŒ–æŒ‡çš„æ˜¯æ”¹å˜ $x$ ä»¥æœ€å°åŒ–æˆ–æœ€å¤§åŒ–æŸä¸ªå‡½æ•° $f(x)$ çš„ä»»åŠ¡**ï¼Œæˆ‘ä»¬é€šå¸¸ä»¥æœ€å°åŒ– $f(x)$ æŒ‡ä»£å¤§å¤šæ•°æœ€ä¼˜åŒ–é—®é¢˜ã€‚

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼ŒæŸå¤±å‡½æ•°æ˜¯ä»£ä»·å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä»£ä»·å‡½æ•°æ˜¯ç›®æ ‡å‡½æ•°çš„ä¸€ç§ç±»å‹ã€‚
- **æŸå¤±å‡½æ•°**ï¼ˆ`loss function`ï¼‰: ç”¨äºå®šä¹‰å•ä¸ªè®­ç»ƒæ ·æœ¬é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®
- **ä»£ä»·å‡½æ•°**ï¼ˆ`cost function`ï¼‰: ç”¨äºå®šä¹‰å•ä¸ªæ‰¹æ¬¡/æ•´ä¸ªè®­ç»ƒé›†æ ·æœ¬é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„ç´¯è®¡è¯¯å·®ã€‚
- **ç›®æ ‡å‡½æ•°**ï¼ˆ`objective function`ï¼‰: æ³›æŒ‡ä»»æ„å¯ä»¥è¢«ä¼˜åŒ–çš„å‡½æ•°ã€‚

**æŸå¤±å‡½æ•°å®šä¹‰**ï¼šæŸå¤±å‡½æ•°æ˜¯ç”¨æ¥é‡åŒ–æ¨¡å‹é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ä¹‹é—´å·®å¼‚çš„ä¸€ä¸ªéè´Ÿå®æ•°å‡½æ•°ï¼Œå…¶å’Œä¼˜åŒ–ç®—æ³•ç´§å¯†è”ç³»ã€‚æ·±åº¦å­¦ä¹ ç®—æ³•ä¼˜åŒ–çš„ç¬¬ä¸€æ­¥ä¾¿æ˜¯ç¡®å®šæŸå¤±å‡½æ•°å½¢å¼ã€‚

æŸå¤±å‡½æ•°å¤§è‡´å¯åˆ†ä¸ºä¸¤ç§ï¼šå›å½’æŸå¤±ï¼ˆé’ˆå¯¹è¿ç»­å‹å˜é‡ï¼‰å’Œåˆ†ç±»æŸå¤±ï¼ˆé’ˆå¯¹ç¦»æ•£å‹å˜é‡ï¼‰ï¼Œå…¶åœ¨æ·±åº¦å­¦ä¹ å®éªŒæµç¨‹ä¸­çš„ä½ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![æ·±åº¦å­¦ä¹ çš„å®éªŒæµç¨‹](../../data/images/loss/define_loss.png)

> å›¾ç‰‡æ¥æºæå®æ¯… 2022 æœºå™¨å­¦ä¹ æš‘æœŸè¯¾ç¨‹-[Machine Learning Pytorch Tutorial](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2022-course-data/Pytorch%20Tutorial%201.pdf)ã€‚

å¸¸ç”¨çš„å‡å°‘æŸå¤±å‡½æ•°çš„ä¼˜åŒ–ç®—æ³•æ˜¯â€œæ¢¯åº¦ä¸‹é™æ³•â€ï¼ˆGradient Descentï¼‰ã€‚

## äºŒï¼Œäº¤å‰ç†µå‡½æ•°-åˆ†ç±»æŸå¤±

äº¤å‰ç†µæŸå¤±(`Cross-Entropy Loss`) åˆç§°ä¸ºå¯¹æ•°ä¼¼ç„¶æŸå¤±(Log-likelihood Loss)ã€å¯¹æ•°æŸå¤±ï¼ŒäºŒåˆ†ç±»æ—¶è¿˜å¯ç§°ä¹‹ä¸ºé€»è¾‘æ–¯è°›å›å½’æŸå¤±(Logistic Loss)ã€‚

### 2.1ï¼Œäº¤å‰ç†µï¼ˆCross-Entropyï¼‰çš„ç”±æ¥
> äº¤å‰ç†µæŸå¤±çš„ç”±æ¥å‚è€ƒæ–‡æ¡£ [AI-EDU: äº¤å‰ç†µæŸå¤±å‡½æ•°](https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/)ã€‚

**1ï¼Œä¿¡æ¯é‡**

ä¿¡æ¯è®ºä¸­ï¼Œä¿¡æ¯é‡çš„è¡¨ç¤ºæ–¹å¼ï¼š
> ã€Šæ·±åº¦å­¦ä¹ ã€‹ï¼ˆèŠ±ä¹¦ï¼‰ä¸­ç§°ä¸ºè‡ªä¿¡æ¯(self-information) ã€‚
> åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ€»æ˜¯ç”¨ $\text{log}$ æ¥è¡¨ç¤ºè‡ªç„¶å¯¹æ•°ï¼Œ**å…¶åº•æ•°**ä¸º $e$ã€‚

$$
I(x_j) = -\log (p(x_j))
$$

- $x_j$ï¼šè¡¨ç¤ºä¸€ä¸ªäº‹ä»¶
- $p(x_j)$ï¼šè¡¨ç¤ºäº‹ä»¶ $x_j$ å‘ç”Ÿçš„æ¦‚ç‡
- $I(x_j)$ï¼šä¿¡æ¯é‡ï¼Œ$x_j$ è¶Šä¸å¯èƒ½å‘ç”Ÿæ—¶ï¼Œå®ƒä¸€æ—¦å‘ç”Ÿåçš„ä¿¡æ¯é‡å°±è¶Šå¤§

**2ï¼Œç†µ**

ä¿¡æ¯é‡åªå¤„ç†å•ä¸ªçš„è¾“å‡ºã€‚æˆ‘ä»¬å¯ä»¥ç”¨ç†µï¼ˆä¹Ÿç§°é¦™å†œç†µ `Shannon entropy`ï¼‰æ¥å¯¹æ•´ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­çš„ä¸ç¡®å®šæ€§æ€»é‡è¿›è¡Œé‡åŒ–:

$$
H(p) = - \sum_j^n p(x_j) \log (p(x_j))
$$

åˆ™ä¸Šé¢çš„é—®é¢˜çš„ç†µæ˜¯ï¼š
$$
\begin{aligned} H(p)&=-[p(x_1) \ln p(x_1) + p(x_2) \ln p(x_2) + p(x_3) \ln p(x_3)] \\\ &=0.7 \times 0.36 + 0.2 \times 1.61 + 0.1 \times 2.30 \\\ &=0.804 \end{aligned}
$$
**3ï¼Œç›¸å¯¹ç†µ(KLæ•£åº¦)**

ç›¸å¯¹ç†µåˆç§° `KL` æ•£åº¦ï¼Œå¦‚æœå¯¹äºåŒä¸€ä¸ªéšæœºå˜é‡ $x$ æœ‰ä¸¤ä¸ªå•ç‹¬çš„æ¦‚ç‡åˆ†å¸ƒ $P(x)$ å’Œ $Q(x)$ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ KL æ•£åº¦ï¼ˆKullback-Leibler (KL) divergenceï¼‰æ¥**è¡¡é‡è¿™ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚**ï¼Œè¿™ä¸ªç›¸å½“äºä¿¡æ¯è®ºèŒƒç•´çš„å‡æ–¹å·®ã€‚

KLæ•£åº¦çš„è®¡ç®—å…¬å¼ï¼š

$$
D_{KL}(p||q)=\sum_{j=1}^m p(x_j) \log {p(x_j) \over q(x_j)}
$$

$m$ ä¸ºäº‹ä»¶çš„æ‰€æœ‰å¯èƒ½æ€§ï¼ˆåˆ†ç±»ä»»åŠ¡ä¸­å¯¹åº”ç±»åˆ«æ•°ç›®ï¼‰ã€‚**$D$ çš„å€¼è¶Šå°ï¼Œè¡¨ç¤º $q$ åˆ†å¸ƒå’Œ $p$ åˆ†å¸ƒè¶Šæ¥è¿‘**ã€‚

**4ï¼Œäº¤å‰ç†µ**

æŠŠä¸Šè¿°äº¤å‰ç†µå…¬å¼å˜å½¢ï¼š

$$
\begin{aligned} D_{KL}(p||q)&=\sum_{j=1}^m p(x_j) \log {p(x_j)} - \sum_{j=1}^m p(x_j) \log q(x_j) \\\ &=- H(p(x)) + H(p,q) \end{aligned}
$$

ç­‰å¼çš„å‰ä¸€éƒ¨åˆ†æ°å·§å°±æ˜¯ $p$ çš„ç†µï¼Œç­‰å¼çš„åä¸€éƒ¨åˆ†ï¼Œå°±æ˜¯**äº¤å‰ç†µ**ï¼ˆæœºå™¨å­¦ä¹ ä¸­ $p$ è¡¨ç¤ºçœŸå®åˆ†å¸ƒï¼ˆç›®æ ‡åˆ†å¸ƒï¼‰ï¼Œ$q$ è¡¨ç¤ºé¢„æµ‹åˆ†å¸ƒï¼‰:

$$
H(p,q) =- \sum_{j=1}^m p(x_j) \log q(x_j)
$$

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°**æ ‡ç­¾å€¼ $y$ å’Œé¢„æµ‹å€¼ $a$** ä¹‹é—´çš„å·®è·ç†µï¼ˆå³**ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„ç›¸ä¼¼æ€§**ï¼‰ï¼Œä½¿ç”¨ KL æ•£åº¦ $D_{KL}(y||a)$ å³å¯ï¼Œä½†å› ä¸ºæ ·æœ¬æ ‡ç­¾å€¼çš„åˆ†å¸ƒé€šå¸¸æ˜¯å›ºå®šçš„ï¼Œå³ $H(a)$ ä¸å˜ã€‚å› æ­¤ï¼Œä¸ºäº†è®¡ç®—æ–¹ä¾¿ï¼Œåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œåªéœ€è¦å…³æ³¨äº¤å‰ç†µå°±å¯ä»¥äº†ã€‚æ‰€ä»¥ï¼Œ**åœ¨æœºå™¨å­¦ä¹ ä¸­ä¸€èˆ¬ç›´æ¥ç”¨äº¤å‰ç†µåšæŸå¤±å‡½æ•°æ¥è¯„ä¼°æ¨¡å‹**ã€‚

$$
loss = \sum_{j = 1}^{m}y_{j}\text{log}(a_{j})
$$

ä¸Šå¼æ˜¯å•ä¸ªæ ·æœ¬çš„æƒ…å†µï¼Œ$m$ **å¹¶ä¸æ˜¯æ ·æœ¬ä¸ªæ•°ï¼Œè€Œæ˜¯åˆ†ç±»ä¸ªæ•°**ã€‚æ‰€ä»¥ï¼Œå¯¹äº**æ‰¹é‡æ ·æœ¬çš„äº¤å‰ç†µæŸå¤±**è®¡ç®—å…¬å¼ï¼ˆå¾ˆé‡è¦!ï¼‰æ˜¯ï¼š

$$
J = -\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^{m} y_{ij} \log a_{ij}
$$

å…¶ä¸­ï¼Œ$n$ æ˜¯æ ·æœ¬æ•°ï¼Œ$m$ æ˜¯åˆ†ç±»æ•°ã€‚
> å…¬å¼å‚è€ƒæ–‡ç« -[AI-EDU: äº¤å‰ç†µæŸå¤±å‡½æ•°](https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/%E7%AC%AC1%E6%AD%A5%20-%20%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/03.2-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html)ï¼Œä½†æ˜¯å°†æ ·æœ¬æ•°æ”¹ä¸º $n$ï¼Œç±»åˆ«æ•°æ”¹ä¸º $m$ã€‚

æœ‰ä¸€ç±»ç‰¹æ®Šé—®é¢˜ï¼Œå°±æ˜¯äº‹ä»¶åªæœ‰ä¸¤ç§æƒ…å†µå‘ç”Ÿçš„å¯èƒ½ï¼Œæ¯”å¦‚â€œæ˜¯ç‹—â€å’Œâ€œä¸æ˜¯ç‹—â€ï¼Œç§°ä¸º $0/1$ åˆ†ç±»æˆ–**äºŒåˆ†ç±»**ã€‚å¯¹äºè¿™ç±»é—®é¢˜ï¼Œç”±äº $m=2ï¼Œy_1=1-y_2ï¼Œa_1=1-a_2$ï¼Œæ‰€ä»¥**äºŒåˆ†ç±»é—®é¢˜çš„å•ä¸ªæ ·æœ¬çš„äº¤å‰ç†µ**å¯ä»¥ç®€åŒ–ä¸ºï¼š

$$
loss =-[y \log a + (1-y) \log (1-a)]
$$

**äºŒåˆ†ç±»å¯¹äºæ‰¹é‡æ ·æœ¬çš„äº¤å‰ç†µ**è®¡ç®—å…¬å¼æ˜¯ï¼š

$$
J= -\frac{1}{n} \sum_{i=1}^n [y_i \log a_i + (1-y_i) \log (1-a_i)]
$$
> ä¸ºä»€ä¹ˆäº¤å‰ç†µçš„ä»£ä»·å‡½æ•°æ˜¯æ±‚å‡å€¼è€Œä¸æ˜¯æ±‚å’Œ?
>  Cross entropy loss is defined as the â€œexpectationâ€ of the probability distribution of a random variable ğ‘‹, and thatâ€™s why we use mean instead of sum. å‚è§[è¿™é‡Œ](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html#cross-entropy)ã€‚

#### 2.1.1ï¼Œç†µã€ç›¸å¯¹ç†µä»¥åŠäº¤å‰ç†µæ€»ç»“

> äº¤å‰ç†µ $H(p, q)$ ä¹Ÿè®°ä½œ $CE(p, q)$ã€$H(P, Q)$ï¼Œå…¶å¦ä¸€ç§è¡¨è¾¾å…¬å¼ï¼ˆå…¬å¼è¡¨è¾¾å½¢å¼è™½ç„¶ä¸ä¸€æ ·ï¼Œä½†æ˜¯æ„ä¹‰ç›¸åŒï¼‰:
> $$H(P, Q)  = -\mathbb{E}_{\textrm{x}\sim p}log(q(x))$$

äº¤å‰ç†µå‡½æ•°å¸¸ç”¨äºé€»è¾‘å›å½’(`logistic regression`)ï¼Œä¹Ÿå°±æ˜¯åˆ†ç±»(`classification`)ã€‚

æ ¹æ®ä¿¡æ¯è®ºä¸­ç†µçš„æ€§è´¨ï¼Œå°†ç†µã€ç›¸å¯¹ç†µï¼ˆKL æ•£åº¦ï¼‰ä»¥åŠäº¤å‰ç†µçš„å…¬å¼æ”¾åˆ°ä¸€èµ·æ€»ç»“å¦‚ä¸‹:

$$\begin{aligned}
H(p) &= -\sum_{j}p(x_j) \log p(x_j) \\
D_{KL}(p \parallel q) &= \sum_{j}p(x_j)\log \frac{p(x_j)}{q(x_j)} = \sum_j (p(x_j)\log p(x_j) - p(x_j) \log q(x_j)) \\
H(p,q) &=  -\sum_j p(x_j)\log q(x_j) \\
\end{aligned} $$

### 2.2ï¼ŒäºŒåˆ†ç±»é—®é¢˜çš„äº¤å‰ç†µ

æŠŠäºŒåˆ†ç±»çš„äº¤å‰ç†µå…¬å¼ 4 åˆ†è§£å¼€ä¸¤ç§æƒ…å†µï¼š
- å½“ $y=1$ æ—¶ï¼Œå³æ ‡ç­¾å€¼æ˜¯ $1$ ï¼Œæ˜¯ä¸ªæ­£ä¾‹ï¼ŒåŠ å·åé¢çš„é¡¹ä¸º: $loss = -\log(a)$
- å½“ $y=0$ æ—¶ï¼Œå³æ ‡ç­¾å€¼æ˜¯ $0$ï¼Œæ˜¯ä¸ªåä¾‹ï¼ŒåŠ å·å‰é¢çš„é¡¹ä¸º $0$: $loss = -\log (1-a)$

æ¨ªåæ ‡æ˜¯é¢„æµ‹è¾“å‡ºï¼Œçºµåæ ‡æ˜¯æŸå¤±å‡½æ•°å€¼ã€‚$y=1$ æ„å‘³ç€å½“å‰æ ·æœ¬æ ‡ç­¾å€¼æ˜¯1ï¼Œå½“é¢„æµ‹è¾“å‡ºè¶Šæ¥è¿‘1æ—¶ï¼ŒæŸå¤±å‡½æ•°å€¼è¶Šå°ï¼Œè®­ç»ƒç»“æœè¶Šå‡†ç¡®ã€‚å½“é¢„æµ‹è¾“å‡ºè¶Šæ¥è¿‘0æ—¶ï¼ŒæŸå¤±å‡½æ•°å€¼è¶Šå¤§ï¼Œè®­ç»ƒç»“æœè¶Šç³Ÿç³•ã€‚æ­¤æ—¶ï¼ŒæŸå¤±å‡½æ•°å€¼å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°å›¾](../../data/images/loss/äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°å›¾.png)

### 2.3ï¼Œå¤šåˆ†ç±»é—®é¢˜çš„äº¤å‰ç†µ

å½“æ ‡ç­¾å€¼ä¸æ˜¯é0å³1çš„æƒ…å†µæ—¶ï¼Œå°±æ˜¯å¤šåˆ†ç±»äº†ã€‚

å‡è®¾å¸Œæœ›æ ¹æ®å›¾ç‰‡åŠ¨ç‰©çš„è½®å»“ã€é¢œè‰²ç­‰ç‰¹å¾ï¼Œæ¥é¢„æµ‹åŠ¨ç‰©çš„ç±»åˆ«ï¼Œæœ‰ä¸‰ç§å¯é¢„æµ‹ç±»åˆ«ï¼šçŒ«ã€ç‹—ã€çŒªã€‚å‡è®¾æˆ‘ä»¬è®­ç»ƒäº†ä¸¤ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œå…¶é¢„æµ‹ç»“æœå¦‚ä¸‹:

**æ¨¡å‹1**:

|é¢„æµ‹å€¼|æ ‡ç­¾å€¼|æ˜¯å¦æ­£ç¡®|
|-----|-----|-------|
|0.3 0.3 0.4|0 0 1ï¼ˆçŒªï¼‰|æ­£ç¡®|
|0.3 0.4 0.4|0 1 0ï¼ˆç‹—ï¼‰|æ­£ç¡®|
|0.1 0.2 0.7|1 0 0ï¼ˆçŒ«ï¼‰|é”™è¯¯|

æ¯è¡Œè¡¨ç¤ºä¸åŒæ ·æœ¬çš„é¢„æµ‹æƒ…å†µï¼Œå…¬å…± 3 ä¸ªæ ·æœ¬ã€‚å¯ä»¥çœ‹å‡ºï¼Œæ¨¡å‹ 1 å¯¹äºæ ·æœ¬ 1 å’Œæ ·æœ¬ 2 ä»¥éå¸¸å¾®å¼±çš„ä¼˜åŠ¿åˆ¤æ–­æ­£ç¡®ï¼Œå¯¹äºæ ·æœ¬ 3 çš„åˆ¤æ–­åˆ™å½»åº•é”™è¯¯ã€‚

**æ¨¡å‹2**:

|é¢„æµ‹å€¼|æ ‡ç­¾å€¼|æ˜¯å¦æ­£ç¡®|
|-----|-----|-------|
|0.1 0.2 0.7|0 0 1ï¼ˆçŒªï¼‰|æ­£ç¡®|
|0.1 0.7 0.2|0 1 0ï¼ˆç‹—ï¼‰|æ­£ç¡®|
|0.3 0.4 0.4|1 0 0ï¼ˆçŒ«ï¼‰|é”™è¯¯|

å¯ä»¥çœ‹å‡ºï¼Œæ¨¡å‹ 2 å¯¹äºæ ·æœ¬ 1 å’Œæ ·æœ¬ 2 åˆ¤æ–­éå¸¸å‡†ç¡®ï¼ˆé¢„æµ‹æ¦‚ç‡å€¼æ›´è¶‹è¿‘äº 1ï¼‰ï¼Œå¯¹äºæ ·æœ¬ 3 è™½ç„¶åˆ¤æ–­é”™è¯¯ï¼Œä½†æ˜¯ç›¸å¯¹æ¥è¯´æ²¡æœ‰é”™å¾—å¤ªç¦»è°±ï¼ˆé¢„æµ‹æ¦‚ç‡å€¼è¿œå°äº 1ï¼‰ã€‚

ç»“åˆå¤šåˆ†ç±»çš„äº¤å‰ç†µæŸå¤±å‡½æ•°å…¬å¼å¯å¾—ï¼Œæ¨¡å‹ 1 çš„äº¤å‰ç†µä¸º:

$$\begin{aligned} 
\text{sample}\ 1\ \text{loss} = -(0\times log(0.3) + 0\times log(0.3) + 1\times log(0.4) = 0.91 \\
\text{sample}\ 1\ \text{loss} = -(0\times log(0.3) + 1\times log(0.4) + 0\times log(0.4) = 0.91 \\
\text{sample}\ 1\ \text{loss} = -(1\times log(0.1) + 0\times log(0.2) + 0\times log(0.7) = 2.30
\end{aligned}$$

å¯¹æ‰€æœ‰æ ·æœ¬çš„ `loss` æ±‚å¹³å‡:

$$
L = \frac{0.91 + 0.91 + 2.3}{3} = 1.37
$$

æ¨¡å‹ 2 çš„äº¤å‰ç†µä¸º:

$$\begin{aligned} 
\text{sample}\ 1\ \text{loss} = -(0\times log(0.1) + 0\times log(0.2) + 1\times log(0.7) = 0.35 \\
\text{sample}\ 1\ \text{loss} = -(0\times log(0.1) + 1\times log(0.7) + 0\times log(0.2) = 0.35 \\
\text{sample}\ 1\ \text{loss} = -(1\times log(0.3) + 0\times log(0.4) + 0\times log(0.4) = 1.20
\end{aligned} $$

å¯¹æ‰€æœ‰æ ·æœ¬çš„ `loss` æ±‚å¹³å‡:

$$
L = \frac{0.35 + 0.35 + 1.2}{3} = 0.63
$$

å¯ä»¥çœ‹åˆ°ï¼Œ0.63 æ¯” 1.37 çš„æŸå¤±å€¼å°å¾ˆå¤šï¼Œè¿™è¯´æ˜é¢„æµ‹å€¼è¶Šæ¥è¿‘çœŸå®æ ‡ç­¾å€¼ï¼Œå³äº¤å‰ç†µæŸå¤±å‡½æ•°å¯ä»¥è¾ƒå¥½çš„æ•æ‰åˆ°æ¨¡å‹ 1 å’Œæ¨¡å‹ 2 é¢„æµ‹æ•ˆæœçš„å·®å¼‚ã€‚**äº¤å‰ç†µæŸå¤±å‡½æ•°å€¼è¶Šå°ï¼Œåå‘ä¼ æ’­çš„åŠ›åº¦è¶Šå°**ã€‚
> å¤šåˆ†ç±»é—®é¢˜è®¡ç®—äº¤å‰ç†µçš„å®ä¾‹æ¥æºäºçŸ¥ä¹æ–‡ç« -[æŸå¤±å‡½æ•°ï½œäº¤å‰ç†µæŸå¤±å‡½æ•°](https://zhuanlan.zhihu.com/p/35709485)ã€‚

### 2.4ï¼ŒPyTorch ä¸­çš„ Cross Entropy

PyTorch ä¸­å¸¸ç”¨çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸º `torch.nn.CrossEntropyLoss`

```python
class torch.nn.CrossEntropyLoss(weight=None, size_average=None,
                                ignore_index=-100, reduce=None, 
                                reduction='elementwise_mean')
```

**1ï¼Œå‡½æ•°åŠŸèƒ½**:

å°†è¾“å…¥ç»è¿‡ `softmax` æ¿€æ´»å‡½æ•°ä¹‹åï¼Œå†è®¡ç®—å…¶ä¸ `target` çš„äº¤å‰ç†µæŸå¤±ã€‚å³è¯¥æ–¹æ³•å°† `nn.LogSoftmax()` å’Œ `nn.NLLLoss()`è¿›è¡Œäº†ç»“åˆã€‚ä¸¥æ ¼æ„ä¹‰ä¸Šçš„äº¤å‰ç†µæŸå¤±å‡½æ•°åº”è¯¥æ˜¯ `nn.NLLLoss()`ã€‚

**2ï¼Œå‚æ•°è§£é‡Š**:

- `weight`(Tensor)- ä¸ºæ¯ä¸ªç±»åˆ«çš„ loss è®¾ç½®æƒå€¼ï¼Œå¸¸ç”¨äºç±»åˆ«ä¸å‡è¡¡é—®é¢˜ã€‚weight å¿…é¡»æ˜¯ float ç±»å‹çš„ tensorï¼Œå…¶é•¿åº¦è¦äºç±»åˆ« `C` ä¸€è‡´ï¼Œå³æ¯ä¸€ä¸ªç±»åˆ«éƒ½è¦è®¾ç½®æœ‰ weightã€‚
- `size_average`(bool)- å½“ reduce=True æ—¶æœ‰æ•ˆã€‚ä¸º True æ—¶ï¼Œè¿”å›çš„ loss ä¸ºå¹³å‡å€¼;ä¸º False æ—¶ï¼Œè¿”å›çš„å„æ ·æœ¬çš„ loss ä¹‹å’Œã€‚
- `reduce`(bool)- è¿”å›å€¼æ˜¯å¦ä¸ºæ ‡é‡ï¼Œé»˜è®¤ä¸º Trueã€‚
- `ignore_index`(int)- å¿½ç•¥æŸä¸€ç±»åˆ«ï¼Œä¸è®¡ç®—å…¶ `loss`ï¼Œå…¶ loss ä¼šä¸º 0ï¼Œå¹¶ä¸”ï¼Œåœ¨é‡‡ç”¨ size_average æ—¶ï¼Œä¸ä¼šè®¡ç®—é‚£ä¸€ç±»çš„ lossï¼Œé™¤çš„æ—¶å€™çš„åˆ†æ¯ä¹Ÿä¸ä¼šç»Ÿè®¡é‚£ä¸€ç±»çš„æ ·æœ¬ã€‚

#### 2.4.1ï¼ŒSoftmax å¤šåˆ†ç±»å‡½æ•°
> æ³¨æ„: Softmax ç”¨ä½œæ¨¡å‹æœ€åä¸€å±‚çš„å‡½æ•°é€šå¸¸å’Œäº¤å‰ç†µä½œæŸå¤±å‡½æ•°é…å¥—æ­é…ä½¿ç”¨ï¼Œåº”ç”¨äºå¤šåˆ†ç±»ä»»åŠ¡ã€‚

å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ `Logistic` å‡½æ•°è®¡ç®—æ ·æœ¬çš„æ¦‚ç‡å€¼ï¼Œä»è€ŒæŠŠæ ·æœ¬åˆ†æˆäº†æ­£è´Ÿä¸¤ç±»ã€‚å¯¹äºå¤šåˆ†ç±»é—®é¢˜ï¼Œåˆ™ä½¿ç”¨ `Softmax` ä½œä¸ºæ¨¡å‹æœ€åä¸€å±‚çš„æ¿€æ´»å‡½æ•°æ¥å°†**å¤šåˆ†ç±»çš„è¾“å‡ºå€¼è½¬æ¢ä¸ºèŒƒå›´åœ¨ [0, 1] å’Œä¸º 1 çš„æ¦‚ç‡åˆ†å¸ƒ**ã€‚

Softmax ä»å­—é¢ä¸Šæ¥è¯´ï¼Œå¯ä»¥åˆ†æˆ soft å’Œ max ä¸¤ä¸ªéƒ¨åˆ†ã€‚max æ•…åæ€è®®å°±æ˜¯æœ€å¤§å€¼çš„æ„æ€ã€‚Softmax çš„æ ¸å¿ƒåœ¨äº softï¼Œè€Œ soft æœ‰è½¯çš„å«ä¹‰ï¼Œä¸ä¹‹ç›¸å¯¹çš„æ˜¯ hard ç¡¬ï¼Œå³ herdmaxã€‚ä¸‹é¢åˆ†å¸ƒæ¼”ç¤ºå°†æ¨¡å‹è¾“å‡ºå€¼**å– max å€¼**å’Œ**å¼•å…¥ Softmax** çš„å¯¹æ¯”æƒ…å†µã€‚

**å–maxå€¼ï¼ˆhardmaxï¼‰**

å‡è®¾æ¨¡å‹è¾“å‡ºç»“æœ $z$ å€¼æ˜¯ $[3,1,-3]$ï¼Œå¦‚æœå– max æ“ä½œä¼šå˜æˆ $[1, 0, 0]$ï¼Œè¿™ç¬¦åˆæˆ‘ä»¬çš„åˆ†ç±»éœ€è¦ï¼Œå³ä¸‰è€…ç›¸åŠ ä¸º1ï¼Œå¹¶ä¸”è®¤ä¸ºè¯¥æ ·æœ¬å±äºç¬¬ä¸€ç±»ã€‚ä½†æ˜¯æœ‰ä¸¤ä¸ªä¸è¶³ï¼š

1. åˆ†ç±»ç»“æœæ˜¯ $[1,0,0]$ï¼Œåªä¿ç•™é 0 å³ 1 çš„ä¿¡æ¯ï¼Œå³éé»‘å³ç™½ï¼Œæ²¡æœ‰å„å…ƒç´ ä¹‹é—´ç›¸å·®å¤šå°‘çš„ä¿¡æ¯ï¼Œå¯ä»¥ç†è§£æ˜¯â€œHard Maxâ€ï¼›
2. max æ“ä½œæœ¬èº«ä¸å¯å¯¼ï¼Œæ— æ³•ç”¨åœ¨åå‘ä¼ æ’­ä¸­ã€‚

**å¼•å…¥Softmax**

`Softmax` åŠ äº†ä¸ª"soft"æ¥æ¨¡æ‹Ÿ max çš„è¡Œä¸ºï¼Œä½†åŒæ—¶åˆä¿ç•™äº†ç›¸å¯¹å¤§å°çš„ä¿¡æ¯ã€‚

$$
a_j = \text{Softmax}(z_j) = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}}
$$

ä¸Šå¼ä¸­:

- $z_j$ æ˜¯å¯¹ç¬¬ $j$ é¡¹çš„åˆ†ç±»åŸå§‹å€¼ï¼Œå³çŸ©é˜µè¿ç®—çš„ç»“æœ
- $z_i$ æ˜¯å‚ä¸åˆ†ç±»è®¡ç®—çš„æ¯ä¸ªç±»åˆ«çš„åŸå§‹å€¼
- $m$ æ˜¯æ€»åˆ†ç±»æ•°
- $a_j$ æ˜¯å¯¹ç¬¬ $j$ é¡¹çš„è®¡ç®—ç»“æœ

å’Œ hardmax ç›¸æ¯”ï¼ŒSoftmax çš„å«ä¹‰å°±åœ¨äºä¸å†å”¯ä¸€çš„ç¡®å®šæŸä¸€ä¸ªæœ€å¤§å€¼ï¼Œè€Œæ˜¯ä¸ºæ¯ä¸ªè¾“å‡ºåˆ†ç±»çš„ç»“æœéƒ½èµ‹äºˆä¸€ä¸ªæ¦‚ç‡å€¼ï¼ˆç½®ä¿¡åº¦ï¼‰ï¼Œè¡¨ç¤ºå±äºæ¯ä¸ªç±»åˆ«çš„å¯èƒ½æ€§ã€‚

ä¸‹å›¾å¯ä»¥å½¢è±¡åœ°è¯´æ˜ Softmax çš„è®¡ç®—è¿‡ç¨‹ã€‚

![Softmaxå·¥ä½œè¿‡ç¨‹](../../data/images/loss/softmax_process.png)

å½“è¾“å…¥çš„æ•°æ® $[z_1,z_2,z_3]$ æ˜¯ $[3, 1, -3]$ æ—¶ï¼ŒæŒ‰ç…§å›¾ç¤ºè¿‡ç¨‹è¿›è¡Œè®¡ç®—ï¼Œå¯ä»¥å¾—å‡ºè¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒæ˜¯ $[0.879,0.119,0.002]$ã€‚å¯¹æ¯” max è¿ç®—å’Œ Softmax çš„ä¸åŒï¼Œå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚

|è¾“å…¥åŸå§‹å€¼|MAXè®¡ç®—|Softmaxè®¡ç®—|
|--------|-------|----------|
|$[3, 1, -3]$|$[1, 0, 0]$|$[0.879, 0.119, 0.002]$|

å¯ä»¥çœ‹å‡º Softmax è¿ç®—ç»“æœä¸¤ä¸ªç‰¹ç‚¹ï¼š

1. ä¸‰ä¸ªç±»åˆ«çš„æ¦‚ç‡ç›¸åŠ ä¸º 1
2. æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡éƒ½å¤§äº 0

ä¸‹é¢æˆ‘å†ç»™å‡º hardmax å’Œ softmax è®¡ç®—çš„ä»£ç å®ç°ã€‚

```python
# example of the argmax of a list of numbers
from numpy import argmax
from numpy import exp

# define data
data = [3, 1, -3]

def hardmax(data):
    """# calculate the argmax of the list"""
    result = argmax(data) 
    return result

def softmax(vector):
    """# calculate the softmax of a vector"""
    e = exp(vector)
    return e / e.sum()

hardmax_result = hardmax(data)
# è¿è¡Œè¯¥ç¤ºä¾‹è¿”å›åˆ—è¡¨ç´¢å¼•å€¼â€œ0â€ï¼Œè¯¥å€¼æŒ‡å‘åŒ…å«åˆ—è¡¨â€œ3â€ä¸­æœ€å¤§å€¼çš„æ•°ç»„ç´¢å¼• [1]ã€‚
print(hardmax(data)) # 0

# convert list of numbers to a list of probabilities
softmax_result = softmax(data) 
print(softmax_result) # report the probabilities
print(sum(softmax_result)) # report the sum of the probabilitie
```

è¿è¡Œä»¥ä¸Šä»£ç åï¼Œè¾“å‡ºç»“æœå¦‚ä¸‹:
> 0
[0.87887824 0.11894324 0.00217852]
1.0

å¾ˆæ˜æ˜¾ç¨‹åºçš„è¾“å‡ºç»“æœå’Œæˆ‘ä»¬æ‰‹åŠ¨è®¡ç®—çš„ç»“æœæ˜¯ä¸€æ ·çš„ã€‚

Pytorch ä¸­çš„ Softmax å‡½æ•°å®šä¹‰å¦‚ä¸‹:

```python
def softmax(x):
    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)
```

`dim=1` ç”¨äº `torch.sum()` å¯¹æ‰€æœ‰åˆ—çš„æ¯ä¸€è¡Œæ±‚å’Œï¼Œ`.view(-1,1)` ç”¨äºé˜²æ­¢å¹¿æ’­ã€‚

### 2.5ï¼Œä¸ºä»€ä¹ˆä¸èƒ½ä½¿ç”¨å‡æ–¹å·®åšä¸ºåˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°ï¼Ÿ

å›å½’é—®é¢˜é€šå¸¸ç”¨å‡æ–¹å·®æŸå¤±å‡½æ•°ï¼Œå¯ä»¥ä¿è¯æŸå¤±å‡½æ•°æ˜¯ä¸ªå‡¸å‡½æ•°ï¼Œå³å¯ä»¥å¾—åˆ°æœ€ä¼˜è§£ã€‚è€Œåˆ†ç±»é—®é¢˜å¦‚æœç”¨å‡æ–¹å·®çš„è¯ï¼ŒæŸå¤±å‡½æ•°çš„è¡¨ç°ä¸æ˜¯å‡¸å‡½æ•°ï¼Œå°±å¾ˆéš¾å¾—åˆ°æœ€ä¼˜è§£ã€‚è€Œäº¤å‰ç†µå‡½æ•°å¯ä»¥ä¿è¯åŒºé—´å†…å•è°ƒã€‚

åˆ†ç±»é—®é¢˜çš„æœ€åä¸€å±‚ç½‘ç»œï¼Œéœ€è¦åˆ†ç±»å‡½æ•°ï¼Œ`Sigmoid` æˆ–è€… `Softmax`ï¼Œå¦‚æœå†æ¥å‡æ–¹å·®å‡½æ•°çš„è¯ï¼Œå…¶æ±‚å¯¼ç»“æœå¤æ‚ï¼Œè¿ç®—é‡æ¯”è¾ƒå¤§ã€‚ç”¨äº¤å‰ç†µå‡½æ•°çš„è¯ï¼Œå¯ä»¥å¾—åˆ°æ¯”è¾ƒç®€å•çš„è®¡ç®—ç»“æœï¼Œä¸€ä¸ªç®€å•çš„å‡æ³•å°±å¯ä»¥å¾—åˆ°åå‘è¯¯å·®ã€‚

## ä¸‰ï¼Œå›å½’æŸå¤±

ä¸åˆ†ç±»é—®é¢˜ä¸åŒï¼Œå›å½’é—®é¢˜è§£å†³çš„æ˜¯**å¯¹å…·ä½“æ•°å€¼çš„é¢„æµ‹**ã€‚è§£å†³å›å½’é—®é¢˜çš„ç¥ç»ç½‘ç»œä¸€èˆ¬åªæœ‰åªæœ‰ä¸€ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œè¿™ä¸ªèŠ‚ç‚¹çš„è¾“å‡ºå€¼å°±æ˜¯é¢„æµ‹å€¼ã€‚

å›å½’é—®é¢˜çš„ä¸€ä¸ªåŸºæœ¬æ¦‚å¿µæ˜¯**æ®‹å·®**æˆ–ç§°ä¸º**é¢„æµ‹è¯¯å·®**ï¼Œç”¨äºè¡¡é‡æ¨¡å‹é¢„æµ‹å€¼ä¸çœŸå®æ ‡è®°çš„é è¿‘ç¨‹åº¦ã€‚å‡è®¾å›å½’é—®é¢˜ä¸­å¯¹åº”äºç¬¬ $i$ ä¸ªè¾“å…¥ç‰¹å¾ $x_i$ çš„**æ ‡ç­¾**ä¸º $y^i = (y_1,y_2,...,y_M)^{\top}$ï¼Œ$M$ ä¸ºæ ‡è®°å‘é‡æ€»ç»´åº¦ï¼Œåˆ™ $l_{t}^{i}$ å³è¡¨ç¤ºæ ·æœ¬ $i$ ä¸Šç¥ç»ç½‘ç»œçš„å›å½’é¢„æµ‹å€¼ ($y^i$) ä¸å…¶æ ·æœ¬æ ‡ç­¾å€¼åœ¨ç¬¬ $t$ ç»´çš„é¢„æµ‹è¯¯å·®(äº¦ç§°æ®‹å·®):

$$
l_{t}^{i} = y_{t}^{i} - \hat{y}_{t}^{i}
$$

å¸¸ç”¨çš„ä¸¤ç§æŸå¤±å‡½æ•°ä¸º $\text{MAE}$ï¼ˆä¹Ÿå« `L1` æŸå¤±ï¼‰ å’Œ $\text{MSE}$ æŸå¤±å‡½æ•°ï¼ˆä¹Ÿå« `L2` æŸå¤±ï¼‰ã€‚


### 3.1ï¼ŒMAE æŸå¤±

å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMean Absolute Errorï¼Œ`MAE`ï¼‰æ˜¯ç”¨äºå›å½’æ¨¡å‹çš„æœ€ç®€å•ä½†æœ€å¼ºå¤§çš„æŸå¤±å‡½æ•°ä¹‹ä¸€ã€‚

å› ä¸ºå­˜åœ¨ç¦»ç¾¤å€¼ï¼ˆä¸å…¶ä½™æ•°æ®å·®å¼‚å¾ˆå¤§çš„å€¼ï¼‰ï¼Œæ‰€ä»¥å›å½’é—®é¢˜å¯èƒ½å…·æœ‰æœ¬è´¨ä¸Šä¸æ˜¯ä¸¥æ ¼é«˜æ–¯åˆ†å¸ƒçš„å˜é‡ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¹³å‡ç»å¯¹è¯¯å·®å°†æ˜¯ä¸€ä¸ªç†æƒ³çš„é€‰æ‹©ï¼Œå› ä¸ºå®ƒæ²¡æœ‰è€ƒè™‘å¼‚å¸¸å€¼çš„æ–¹å‘ï¼ˆä¸åˆ‡å®é™…çš„é«˜æ­£å€¼æˆ–è´Ÿå€¼ï¼‰ã€‚

é¡¾åæ€ä¹‰ï¼ŒMAE æ˜¯**ç›®æ ‡å€¼å’Œé¢„æµ‹å€¼ä¹‹å·®çš„ç»å¯¹å€¼ä¹‹å’Œ**ã€‚$n$ æ˜¯æ•°æ®é›†ä¸­æ•°æ®ç‚¹çš„æ€»æ•°ï¼Œå…¶å…¬å¼å¦‚ä¸‹:
$$
\text{MAE loss} = \frac{1}{n}\sum_{i=1}^{N}\sum_{t=1}^{M} |y_{t}^{i} - \hat{y}_{t}^{i}|
$$

### 3.2ï¼ŒMSE æŸå¤±

å‡æ–¹è¯¯å·®ï¼ˆMean Square Error, `MSE`ï¼‰å‡ ä¹æ˜¯æ¯ä¸ªæ•°æ®ç§‘å­¦å®¶åœ¨å›å½’æŸå¤±å‡½æ•°æ–¹é¢çš„åå¥½ï¼Œè¿™æ˜¯å› ä¸º**å¤§å¤šæ•°å˜é‡éƒ½å¯ä»¥å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒ**ã€‚

å‡æ–¹è¯¯å·®è®¡ç®—æ–¹æ³•æ˜¯æ±‚**é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´è·ç¦»çš„å¹³æ–¹å’Œ**ã€‚é¢„æµ‹å€¼å’ŒçœŸå®å€¼è¶Šæ¥è¿‘ï¼Œä¸¤è€…çš„å‡æ–¹å·®å°±è¶Šå°ã€‚å…¬å¼å¦‚ä¸‹:

$$
\text{MSE loss} = \frac{1}{n}\sum_{i=1}^{N}\sum_{t=1}^{M} (y_{t}^{i} - \hat{y}_{t}^{i})^2
$$

### 3.3ï¼Œ`Huber` æŸå¤±

MAE å’Œ MSE æŸå¤±ä¹‹é—´çš„æ¯”è¾ƒäº§ç”Ÿä»¥ä¸‹ç»“æœï¼š

1. **MAE æŸå¤±æ¯” MSE æŸå¤±æ›´ç¨³å¥**ã€‚ä»”ç»†æŸ¥çœ‹å…¬å¼ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°å¦‚æœé¢„æµ‹å€¼å’Œå®é™…å€¼ä¹‹é—´çš„å·®å¼‚å¾ˆå¤§ï¼Œä¸ MAE ç›¸æ¯”ï¼ŒMSE æŸå¤±ä¼šæ”¾å¤§æ•ˆæœã€‚ ç”±äº MSE ä¼šå±ˆæœäºå¼‚å¸¸å€¼ï¼Œå› æ­¤ MAE æŸå¤±å‡½æ•°æ˜¯æ›´ç¨³å¥çš„æŸå¤±å‡½æ•°ã€‚

2. **MAE æŸå¤±ä¸å¦‚ MSE æŸå¤±ç¨³å®š**ã€‚ç”±äº MAE æŸå¤±å¤„ç†çš„æ˜¯è·ç¦»å·®å¼‚ï¼Œå› æ­¤ä¸€ä¸ªå°çš„æ°´å¹³å˜åŒ–éƒ½å¯èƒ½å¯¼è‡´å›å½’çº¿æ³¢åŠ¨å¾ˆå¤§ã€‚åœ¨å¤šæ¬¡è¿­ä»£ä¸­å‘ç”Ÿçš„å½±å“å°†å¯¼è‡´è¿­ä»£ä¹‹é—´çš„æ–œç‡å‘ç”Ÿæ˜¾è‘—å˜åŒ–ã€‚æ€»ç»“å°±æ˜¯ï¼ŒMSE å¯ä»¥ç¡®ä¿å›å½’çº¿è½»å¾®ç§»åŠ¨ä»¥å¯¹æ•°æ®ç‚¹è¿›è¡Œå°å¹…è°ƒæ•´ã€‚
3. **MAE æŸå¤±æ›´æ–°çš„æ¢¯åº¦å§‹ç»ˆç›¸åŒ**ã€‚å³ä½¿å¯¹äºå¾ˆå°çš„æŸå¤±å€¼ï¼Œæ¢¯åº¦ä¹Ÿå¾ˆå¤§ã€‚è¿™æ ·ä¸åˆ©äºæ¨¡å‹çš„å­¦ä¹ ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªç¼ºé™·ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å˜åŒ–çš„å­¦ä¹ ç‡ï¼Œåœ¨æŸå¤±æ¥è¿‘æœ€å°å€¼æ—¶é™ä½å­¦ä¹ ç‡ã€‚
4. **MSE æŸå¤±çš„æ¢¯åº¦éšæŸå¤±å¢å¤§è€Œå¢å¤§ï¼Œè€ŒæŸå¤±è¶‹äº0æ—¶åˆ™ä¼šå‡å°**ã€‚å…¶ä½¿ç”¨å›ºå®šçš„å­¦ä¹ ç‡ä¹Ÿå¯ä»¥æœ‰æ•ˆæ”¶æ•›ã€‚

Huber Loss ç»“åˆäº† MAE çš„ç¨³å¥æ€§å’Œ MSE çš„ç¨³å®šæ€§ï¼Œæœ¬è´¨ä¸Šæ˜¯ MAE å’Œ MSE æŸå¤±ä¸­æœ€å¥½çš„ã€‚**å¯¹äºå¤§è¯¯å·®ï¼Œå®ƒæ˜¯çº¿æ€§çš„ï¼Œå¯¹äºå°è¯¯å·®ï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯äºŒæ¬¡çš„**ã€‚

Huber Loss çš„ç‰¹å¾åœ¨äºå‚æ•° $\delta$ã€‚å½“ $|y âˆ’ \hat{y}|$ å°äºä¸€ä¸ªäº‹å…ˆæŒ‡å®šçš„å€¼ $\delta $ æ—¶ï¼Œå˜ä¸ºå¹³æ–¹æŸå¤±ï¼Œå¤§äº $\delta $ æ—¶ï¼Œåˆ™å˜æˆç±»ä¼¼äºç»å¯¹å€¼æŸå¤±ï¼Œå› æ­¤å…¶æ˜¯æ¯”è¾ƒrobust çš„æŸå¤±å‡½æ•°ã€‚å…¶å®šä¹‰å¦‚ä¸‹:

$$\text{Huber loss} = \left \lbrace \begin{matrix}
\frac12[y_{t}^{i} - \hat{y}_{t}^{i}]^2 & |y_{t}^{i} - \hat{y}_{t}^{i}| \leq \delta \\ 
\delta|y_{t}^{i} - \hat{y}_{t}^{i}| - \frac12\delta^2 & |y_{t}^{i} - \hat{y}_{t}^{i})| > \delta
\end{matrix}\right.$$

ä¸‰ç§å›å½’æŸå¤±å‡½æ•°çš„æ›²çº¿å›¾æ¯”è¾ƒå¦‚ä¸‹ï¼š

![loss_for_regression](../../data/images/loss/loss_for_regression.png)
> ä»£ç æ¥æº [Loss Function Plot.ipynb](https://nbviewer.org/github/massquantity/Loss-Functions/blob/master/Loss%20Function%20Plot.ipynb)ã€‚

ä¸‰ç§å›å½’æŸå¤±å‡½æ•°çš„å…¶ä»–å½¢å¼å®šä¹‰å¦‚ä¸‹:

![three_regression_loss](../../data/images/activation_function/three_regression_loss.png)

### 3.4ï¼Œä»£ç å®ç°

ä¸‹é¢æ˜¯ä¸‰ç§å›å½’æŸå¤±å‡½æ•°çš„ python ä»£ç å®ç°ï¼Œä»¥åŠå¯¹åº”çš„ `sklearn` åº“çš„å†…ç½®å‡½æ•°ã€‚

```python
# true: Array of true target variable
# pred: Array of predictions
def mse(true, pred):
    return np.sum((true - pred)**2)

def mae(true, pred):
    return np.sum(np.abs(true - pred))

def huber(true, pred, delta):
    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2),delta*np.abs(true - pred) - 0.5*(delta**2))

    return np.sum(loss)

# also available in sklearn
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
```

## å‚è€ƒèµ„æ–™

1. [ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ -22.11. Information Theoryã€‹](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html#cross-entropy)
2. [æŸå¤±å‡½æ•°ï½œäº¤å‰ç†µæŸå¤±å‡½æ•°](https://zhuanlan.zhihu.com/p/35709485)
3. [AI-EDU: äº¤å‰ç†µæŸå¤±å‡½æ•°](https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/%E7%AC%AC1%E6%AD%A5%20-%20%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/03.2-%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.html)
4. [å¸¸è§å›å½’å’Œåˆ†ç±»æŸå¤±å‡½æ•°æ¯”è¾ƒ](https://www.cnblogs.com/massquantity/p/8964029.html)
5. ã€ŠPyTorch_tutorial_0.0.5_ä½™éœ†åµ©ã€‹
6. https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html
7. [ä¸€æ–‡è¯¦è§£Softmaxå‡½æ•°](https://zhuanlan.zhihu.com/p/105722023)
8. [AI-EDU: å¤šåˆ†ç±»å‡½æ•°](https://microsoft.github.io/ai-edu/%E5%9F%BA%E7%A1%80%E6%95%99%E7%A8%8B/A2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/en-us/Step3%20-%20LinearClassification/07.1-%E5%A4%9A%E5%88%86%E7%B1%BB%E5%87%BD%E6%95%B0.html#711-softmax)
