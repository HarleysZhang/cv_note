目录
---
[toc]

## 一，欧氏距离与余弦相似度（cos距离）
> [专题-机器学习实践](https://github.com/imhuay/Algorithm_Interview_Notes-Chinese/blob/master/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/A-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5.md)
[余弦相似度 | 文本分析：基础](https://zhuanlan.zhihu.com/p/41420179)
### 1.1，余弦相似度
通过对两个文本分词，`TF-IDF` 算法向量化，利用空间中两个向量的夹角，来判断这两个向量的相似程度：(`计算夹角的余弦，取值 0-1`)
+ 当两个向量夹角越大，距离越远，最大距离就是两个向量夹角 180°；
+ 夹角越小，距离越近，最小距离就是两个向量夹角 0°，完全重合。
+ 夹角越小相似度越高，但由于有可能一个文章的特征向量词特别多导致整个向量维度很高，使得计算的代价太大不适合大数据量的计算。

**计算两个向量a、b的夹角余弦：**
我们知道，余弦定理：$cos(\theta) = \frac {a^2+b^2+c^2}{2ab}$ ，由此推得两个向量夹角余弦的计算公式如下：
$$cos(\theta) = \frac {ab}{||a|| \times ||b||} = \frac {x_{1}x_{2}+y_1y_2}{\sqrt{x^2_1+y^2_1}\sqrt{x^2_2+y^2_2}}$$
（分子就是两个向量的内积，分母是两个向量的模长乘积）
### 1.2，欧式距离
在欧几里得空间中，欧式距离其实就是向量空间中两点之间的距离。点 $x = (x_{1}, ..., x_{n})$ 和 $y = (y_{1}, ..., y_{n})$ 之间得欧氏距离计算公式如下：
$$d(x,y) = \sqrt {((x_{1}-y_{1})^{2} + (x_{2}-y_{2})^{2} + ... + (x_{n}-y_{n})^{2})}$$
### 1.3，余弦相似度和欧氏距离的区别
+ 欧式距离和余弦相似度都能度量 `2` 个向量之间的相似度
+ 放到向量空间中看，欧式距离衡量`两点之间`的直线距离，而余弦相似度计算的是`两个向量`之间的夹角
+ 没有归一化时，欧式距离的范围是 `[0, +∞]`，而余弦相似度的范围是 `[-1, 1]`；余弦距离是计算相似程度，而欧氏距离计算的是相同程度（对应值的相同程度）
+ 归一化的情况下，可以将空间想象成一个超球面（三维），欧氏距离就是球面上两点的直线距离，而向量余弦值等价于两点的球面距离，本质是一样。
## 二，混淆矩阵与 PR
混淆矩阵表格如下：
|名称|定义|
|---|---|
|True Positive(真正例, `TP`)|将正类预测为正类数|
|True Negative(真负例, `TN`)|将负类预测为负类数|
|False Positive(假正例, `FP`)|将负类预测为正类数 → 误报 (Type I error)|
|False Negative(假负例子, `FN`)|将正类预测为负类数 → 漏报 (Type II error)|

查准率 `P` 与查全率 `R` 的计算公式如下：
+ 查准率（准确率）`P = TP/(TP+FP)`
+ 查全率（召回率）`R = TP/(TP+FN)`

**准确率描述了模型有多准**，即在预测为正例的结果中，有多少是真正例；**召回率则描述了模型有多全**，即在为真的样本中，有多少被我们的模型预测为正例。
## 三，均方误差和方差、标准差
**1，均方误差（MSE，mean squared error）与均方根误差(RMSE)**

均方误差是预测值与真实值之差的平方和的平均值，即误差平方和的平均数。计算公式形式上接近方差，它的开方叫均方根误差 `RMSE`，均方根误差才和标准差形式上接近。
计算公式如下：
$$\frac{1}{n} \sum_{i=1}^{n}[f(x_i)-y_i]^2$$
在机器学习中均方误差常用作**预测和回归问题的损失函数**，均方误差越小，说明模型预测的越准确，反之则越不准确。

**2，方差(variance)与标准差**

方差是在概率论和统计学中衡量随机变量或一组数据时离散程度的度量，在统计描述和概率分布中各有不同的定义，并有不同的公式。概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。**统计中的方差（样本方差）是样本实际值与实际值的总体平均值之差的平方和的平均值**，即将各个误差之平方（而非取绝对值，使之肯定为正数）相加之后再除以总数。**总体方差**计算公式如下：
$$\sigma ^2 = \frac{\sum_{i=1}^{N}(X_{i}-\mu)^2}{N}$$
公式解析：
1. 因为和样本数无关，所以分母为样本数
2. 累加每个值和均值差值的平方，对应于每个值相对于均值的偏差，对应于离散程度，平方是对离散程度的加剧，同时能让差值总为正数，以符合偏差的概念意义
3. $\sigma$ 的平方表示总体方差，$X$ 表示变量，$\mu $ 表示总体的均值，$N$ 表示总体样本数量。

由于方差是数据的平方，与检测值本身相差太大，难以直观的衡量，所以常用方差开根号换算回来，就成了标准差（Standard Deviation）用$\sigma$ 表示，标准差计算公式如下：
$$\sigma = \sqrt{\frac{\sum_{i=1}^{N}(X_{i}-\mu)^2}{N}}$$
**3，样本方差**

在实际项目中，总体均值难以得到时，应用样本统计量替代总体参数，经校正后，样本方差的计算公式如下：
> 样本方差是指总体各单位变量值与其算术平均数的离差平方的平均数。样本方差的意义是用来估计总体方差（统计术语：样本方差是对总体方差的无偏估计）。

$$\sigma ^2 = \frac{\sum_{i=1}^{n-1}(X_{i}-\overline{x_{i}..x_{n}})^2}{n-1}$$
$\overline{x_{i}..x_{n}}$ 表示样本均值公式分母由总体方差的 `N` 变为了 `n-1`，使得样本方差更能反映总体方差。
## 四，如何处理数据中的缺失值
可以分为以下 2 种情况：
+ **缺失值较多**：直接舍弃该列特征，否则可能会带来较大噪声，从而对结果造成不良影响。
+ **缺失值较少**：当缺失值较少（`< 10%`）时，可以考虑对缺失值进行填充，有几下几种填充策略：
  + 用一个**异常值**填充（比如 0 ），缺失值作为一个特征处理：`data.fillna(0)`
  + 用**均值|条件均值**填充：`data.fillna(data.mean())`
  + 用**相邻数据**填充：`data.fillna(method='pad')`，`data.fillna(method='bfill')`
  + **插值**：`data.interpolate()`
  + **拟合**：简单来说，就是将缺失值也作为一个预测问题来处理：将数据分为正常数据和缺失数据，对有值的数据采用`随机森林`等方法拟合，然后对有缺失值的数据进行预测，用预测的值来填充
## 五，机器学习项目流程
1. 数学抽象
2. 数据获取
3. 预处理与特征选择
4. 模型训练与调优
5. 模型诊断
6. 模型融合/集成
7. 上线运行
## 六，数据清洗与特征处理
> [机器学习中的数据清洗与特征处理综述](https://tech.meituan.com/2015/02/10/machinelearning-data-feature-process.html)
美团的这篇综述文章总结得不错，虽然缺少实例不容易直观理解，但是对于我这个初学者来说也足够了，本章内容几乎都来自于参考文章。
### 5.1，清洗标注数据
清洗标注数据的方法，主要是是数据采样和样本过滤。
+ `数据采样`：对于分类问题：选取正例，负例。对于回归问题，需要采集数据。对于采样得到的文本，根据需要设定样本权重，当模型不能使用全部的数据来训练时，需要对数据进行采样，设定一定的采样率。采样的方法包括随机采样，固定比例采样等方法。
+ 样本过滤：1.结合业务情况进行数据的过滤，例如去除crawler抓取，spam，作弊等数据。 - 2.异常点检测，采用异常点检测算法对样本进行分析，常用的异常点检测算法包括 - 偏差检测，例如聚类，最近邻等。
### 5.2，特征分类
根据不同的分类方法，可以将特征分为：
+ `Low level` 特征和 `High level` 特征
+ 稳定特征与动态特征。
+ 二值特征、连续特征、枚举特征

`Low level` 特征是较低级别的特征，主要是原始特征，不需要或者需要很少的人工处理和干预，例如文本中的词向量特征，图像特征中的像素点大小，用户 `id`，商品 id等。High level 特征是经过比较复杂的处理，结合部分业务逻辑或者规则、模型得到的特征，例如人工打分，模型打分等特征，可以用于较复杂的非线性模型。Low level 比较针对性，覆盖面小。长尾样本的预测值主要受 high level 特征影响。 高频样本的预测值主要受 low level 特征影响。

`稳定特征` 是变化频率较少的特征，例如评价平均分，团购单价价格等，在较长时间段内数值都不会发生变化。动态特征是更新变化比较频繁的特征，有些甚至是实时计算得到的特征，例如距离特征，2 小时销量等特征。或者叫做实时特征和非实时特征。针对两类特征的不同可以针对性地设计特征存储和更新方式，例如对于稳定特征，可以建入索引，较长时间更新一次，如果做缓存的话，缓存的时间可以较长。对于动态特征，需要实时计算或者准实时地更新数据，如果做缓存的话，缓存过期时间需要设置的较短。

`二值特征主要是 0/1 特征`，即特征只取两种值：`0 或者 1`，例如`用户 id 特征`：目前的 id 是否是某个特定的 id，`词向量特征`：某个特定的词是否在文章中出现等等。连续值特征是取值为有理数的特征，特征取值个数不定，例如距离特征，特征取值为是0~正无穷。枚举值特征主要是特征有固定个数个可能值，例如今天周几，只有7个可能值：周1，周2，…，周日。在实际的使用中，我们可能对不同类型的特征进行转换，例如将枚举特征或者连续特征处理为二值特征。枚举特征处理为二值特征技巧：将枚举特征映射为多个特征，每个特征对应一个特定枚举值，例如今天周几，可以把它转换成7个二元特征：今天是否是周一，今天是否是周二，…，今天是否是周日。连续值处理为二值特征方法：先将连续值离散化（后面会介绍如何离散化)，再将离散化后的特征切分为N个二元特征，每个特征代表是否在这个区间内。
### 5.3，特征处理与分析
对特征进行分类后，需要对特征进行处理，`常用的特征处理方法`如下：
+ 特征归一化，离散化，缺省值处理
+ 特征降维方法
+ 特征选择方法

**特征归一化**。在有些算法中，例如线性模型或者距离相关的模型（聚类模型、knn 模型等），特征值的取值范围会对最终的结果产生较大影响，例如输入数据有两种不同的特征，其中的二元特征取值范围 `[0, 1]`，而距离特征取值可能是 [0，正无穷]，两种特征取值范围不一致，导致模型可能会偏向于取值范围较大额特征，为了平衡取值范围不一致的特征，需要对特征进行归一化处理，将特征值取值归一化到 [0,1] 区间，常用的归一化方法包括：
1. `函数归一化`，通过映射函数将特征取值映射到［0，1］区间，例如最大最小值归一化方法，是一种线性的映射。还有通过非线性函数的映射，例如 `log` 函数等。
2. `分维度归一化`，可以使用最大最小归一化方法，但是最大最小值选取的是所属类别的最大最小值，即使用的是局部最大最小值，不是全局的最大最小值。
3. `排序归一化`，不管原来的特征取值是什么样的，将特征按大小排序，根据特征所对应的序给予一个新的值。

**离散化**。在上面介绍过连续值的取值空间可能是无穷的，为了便于表示和在模型中处理，需要对连续值特征进行离散化处理。常用的离散化方法包括等值划分和等量划分。
1. `等值划分`，是将特征按照值域进行均分，每一段内的取值等同处理。例如某个特征的取值范围为 [0，10]，我们可以将其划分为10段，[0，1)，[1，2)，…，[9，10)。
2. `等量划分`，是根据样本总数进行均分，每段等量个样本划分为 1 段。例如距离特征，取值范围［0，3000000］，现在需要切分成 10 段，如果按照等比例划分的话，会发现绝大部分样本都在第 1 段中。使用等量划分就会避免这种问题，最终可能的切分是[0，100)，[100，300)，[300，500)，..，[10000，3000000]，前面的区间划分比较密，后面的比较稀疏。

**缺省值处理**。有些特征可能因为无法采样或者没有观测值而缺失，例如距离特征，用户可能禁止获取地理位置或者获取地理位置失败，此时需要对这些特征做特殊的处理，赋予一个缺省值。缺省值如何赋予，也有很多种方法。例如`单独表示，众数，平均值等`。
## 参考资料
+ [机器学习中的数据清洗与特征处理综述](https://tech.meituan.com/2015/02/10/machinelearning-data-feature-process.html)
+ [02.[必读]均值、方差、标准差](https://zhuanlan.zhihu.com/p/35435231)
