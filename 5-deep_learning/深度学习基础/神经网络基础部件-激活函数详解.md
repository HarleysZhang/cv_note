- [æ¿€æ´»å‡½æ•°æ¦‚è¿°](#æ¿€æ´»å‡½æ•°æ¦‚è¿°)
  - [å‰è¨€](#å‰è¨€)
  - [æ¿€æ´»å‡½æ•°å®šä¹‰](#æ¿€æ´»å‡½æ•°å®šä¹‰)
  - [æ¿€æ´»å‡½æ•°æ€§è´¨](#æ¿€æ´»å‡½æ•°æ€§è´¨)
- [Sigmoid å‹å‡½æ•°](#sigmoid-å‹å‡½æ•°)
  - [Sigmoid å‡½æ•°](#sigmoid-å‡½æ•°)
  - [Tanh å‡½æ•°](#tanh-å‡½æ•°)
- [ReLU å‡½æ•°åŠå…¶å˜ä½“](#relu-å‡½æ•°åŠå…¶å˜ä½“)
  - [ReLU å‡½æ•°](#relu-å‡½æ•°)
  - [Leaky ReLU/PReLU/ELU/Softplus å‡½æ•°](#leaky-relupreluelusoftplus-å‡½æ•°)
- [Swish å‡½æ•°](#swish-å‡½æ•°)
- [æ¿€æ´»å‡½æ•°æ€»ç»“](#æ¿€æ´»å‡½æ•°æ€»ç»“)
- [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

> æœ¬æ–‡åˆ†æäº†æ¿€æ´»å‡½æ•°å¯¹äºç¥ç»ç½‘ç»œçš„å¿…è¦æ€§ï¼ŒåŒæ—¶è®²è§£äº†å‡ ç§å¸¸è§çš„æ¿€æ´»å‡½æ•°çš„åŸç†ï¼Œå¹¶ç»™å‡ºç›¸å…³å…¬å¼ã€ä»£ç å’Œç¤ºä¾‹å›¾ã€‚

## æ¿€æ´»å‡½æ•°æ¦‚è¿°

### å‰è¨€

äººå·¥ç¥ç»å…ƒ(Artificial Neuron)ï¼Œç®€ç§°ç¥ç»å…ƒ(Neuron)ï¼Œæ˜¯æ„æˆç¥ç»ç½‘ç»œçš„åŸºæœ¬å•å…ƒï¼Œå…¶ä¸»è¦æ˜¯æ¨¡æ‹Ÿç”Ÿç‰©ç¥ç»å…ƒçš„ç»“æ„å’Œç‰¹æ€§ï¼Œæ¥æ”¶ä¸€ç»„è¾“å…¥ä¿¡å·å¹¶äº§ç”Ÿè¾“å‡ºã€‚ç”Ÿç‰©ç¥ç»å…ƒä¸äººå·¥ç¥ç»å…ƒçš„å¯¹æ¯”å›¾å¦‚ä¸‹æ‰€ç¤ºã€‚

![neuron](./images/activation_function/neuron.png)

ä»æœºå™¨å­¦ä¹ çš„è§’åº¦æ¥çœ‹ï¼Œç¥ç»ç½‘ç»œå…¶å®å°±æ˜¯ä¸€ä¸ª**éçº¿æ€§æ¨¡å‹**ï¼Œå…¶åŸºæœ¬ç»„æˆå•å…ƒä¸ºå…·æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°çš„ç¥ç»å…ƒï¼Œé€šè¿‡å¤§é‡ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥ï¼Œä½¿å¾—å¤šå±‚ç¥ç»ç½‘ç»œæˆä¸ºä¸€ç§é«˜åº¦éçº¿æ€§çš„æ¨¡å‹ã€‚**ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥æƒé‡å°±æ˜¯éœ€è¦å­¦ä¹ çš„å‚æ•°**ï¼Œå…¶å¯ä»¥åœ¨æœºå™¨å­¦ä¹ çš„æ¡†æ¶ä¸‹é€šè¿‡**æ¢¯åº¦ä¸‹é™æ–¹æ³•**æ¥è¿›è¡Œå­¦ä¹ ã€‚
> æ·±åº¦å­¦ä¹ ä¸€èˆ¬æŒ‡çš„æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ³›æŒ‡ç½‘ç»œå±‚æ•°åœ¨ä¸‰å±‚æˆ–è€…ä¸‰å±‚ä»¥ä¸Šçš„ç¥ç»ç½‘ç»œç»“æ„ã€‚

### æ¿€æ´»å‡½æ•°å®šä¹‰

æ¿€æ´»å‡½æ•°ï¼ˆä¹Ÿç§°â€œéçº¿æ€§æ˜ å°„å‡½æ•°â€ï¼‰ï¼Œæ˜¯æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­å¿…ä¸å¯å°‘çš„ç½‘ç»œå±‚ã€‚

å‡è®¾ä¸€ä¸ªç¥ç»å…ƒæ¥æ”¶ $D$ ä¸ªè¾“å…¥ $x_1, x_2,â‹¯, x_D$ï¼Œä»¤å‘é‡ $x = [x_1;x_2;â‹¯;x_ğ·]$ æ¥è¡¨ç¤ºè¿™ç»„è¾“å…¥ï¼Œå¹¶ç”¨å‡€è¾“å…¥(Net Input) $z \in \mathbb{R}$ è¡¨ç¤ºä¸€ä¸ªç¥ç»å…ƒæ‰€è·å¾—çš„è¾“å…¥ä¿¡å· $x$ çš„åŠ æƒå’Œ:

$$
z = \sum_{d=1}^{D} w_{d}x_{d} + b = w^\top x + b
$$

å…¶ä¸­ $w = [w_1;w_2;â‹¯;w_ğ·]\in \mathbb{R}^D$ æ˜¯ $D$ ç»´çš„æƒé‡çŸ©é˜µï¼Œ$b \in \mathbb{R}$ æ˜¯åç½®å‘é‡ã€‚

ä»¥ä¸Šå…¬å¼å…¶å®å°±æ˜¯**å¸¦æœ‰åç½®é¡¹çš„çº¿æ€§å˜æ¢**ï¼ˆç±»ä¼¼äºæ”¾å°„å˜æ¢ï¼‰ï¼Œæœ¬è´¨ä¸Šè¿˜æ˜¯å±äºçº¿å½¢æ¨¡å‹ã€‚ä¸ºäº†è½¬æ¢æˆéçº¿æ€§æ¨¡å‹ï¼Œæˆ‘ä»¬åœ¨å‡€è¾“å…¥ $z$ åæ·»åŠ ä¸€ä¸ª**éçº¿æ€§å‡½æ•°** $f$ï¼ˆå³æ¿€æ´»å‡½æ•°ï¼‰ã€‚

$$a = f(z)$$

ç”±æ­¤ï¼Œå…¸å‹çš„ç¥ç»å…ƒç»“æ„å¦‚ä¸‹æ‰€ç¤º:
![å…¸å‹çš„ç¥ç»å…ƒæ¶æ„](./images/activation_function/å…¸å‹çš„ç¥ç»å…ƒæ¶æ„.png)

### æ¿€æ´»å‡½æ•°æ€§è´¨

ä¸ºäº†å¢å¼ºç½‘ç»œçš„è¡¨ç¤ºèƒ½åŠ›å’Œå­¦ä¹ èƒ½åŠ›ï¼Œæ¿€æ´»å‡½æ•°éœ€è¦å…·å¤‡ä»¥ä¸‹å‡ ç‚¹æ€§è´¨:
1. **è¿ç»­å¹¶å¯å¯¼(å…è®¸å°‘æ•°ç‚¹ä¸Šä¸å¯å¯¼)çš„éçº¿æ€§å‡½æ•°**ã€‚å¯å¯¼çš„æ¿€æ´»å‡½æ•° å¯ä»¥ç›´æ¥åˆ©ç”¨æ•°å€¼ä¼˜åŒ–çš„æ–¹æ³•æ¥å­¦ä¹ ç½‘ç»œå‚æ•°ã€‚
2. æ¿€æ´»å‡½æ•°åŠå…¶å¯¼å‡½æ•°è¦**å°½å¯èƒ½çš„ç®€å•**ï¼Œæœ‰åˆ©äºæé«˜ç½‘ç»œè®¡ç®—æ•ˆç‡ã€‚
3. æ¿€æ´»å‡½æ•°çš„å¯¼å‡½æ•°çš„**å€¼åŸŸè¦åœ¨ä¸€ä¸ªåˆé€‚çš„åŒºé—´å†…**ï¼Œä¸èƒ½å¤ªå¤§ä¹Ÿä¸èƒ½å¤ªå°ï¼Œå¦åˆ™ä¼šå½±å“è®­ç»ƒçš„æ•ˆç‡å’Œç¨³å®šæ€§.

## Sigmoid å‹å‡½æ•°

Sigmoid å‹å‡½æ•°æ˜¯æŒ‡ä¸€ç±» S å‹æ›²çº¿å‡½æ•°ï¼Œä¸ºä¸¤ç«¯é¥±å’Œå‡½æ•°ã€‚å¸¸ç”¨çš„ Sigmoid å‹å‡½æ•°æœ‰ Logistic å‡½æ•°å’Œ Tanh å‡½æ•°ã€‚
> ç›¸å…³æ•°å­¦çŸ¥è¯†: å¯¹äºå‡½æ•° $f(x)$ï¼Œè‹¥ $x \to âˆ’\infty$ æ—¶ï¼Œå…¶å¯¼æ•° ${f}'\to 0$ï¼Œåˆ™ç§°å…¶ä¸ºå·¦é¥±å’Œã€‚è‹¥ $x \to +\infty$ æ—¶ï¼Œå…¶å¯¼æ•° ${f}'\to 0$ï¼Œåˆ™ç§°å…¶ä¸ºå³é¥±å’Œã€‚å½“åŒæ—¶æ»¡è¶³å·¦ã€å³é¥±å’Œæ—¶ï¼Œå°±ç§°ä¸ºä¸¤ç«¯é¥±å’Œã€‚

### Sigmoid å‡½æ•°

å¯¹äºä¸€ä¸ªå®šä¹‰åŸŸåœ¨ $\mathbb{R}$ ä¸­çš„è¾“å…¥ï¼Œ`sigmoid` å‡½æ•°å°†è¾“å…¥å˜æ¢ä¸ºåŒºé—´ `(0, 1)` ä¸Šçš„è¾“å‡ºï¼ˆsigmoid å‡½æ•°å¸¸è®°ä½œ $\sigma(x)$ï¼‰:

$$
\sigma(x) = \frac{1}{1 + exp(-x)}
$$

sigmoid å‡½æ•°çš„å¯¼æ•°å…¬å¼å¦‚ä¸‹æ‰€ç¤º:

$$
\frac{\mathrm{d} }{\mathrm{d} x}\text{sigmoid}(x) = \frac{exp(-x)}{(1+exp(-x))^2} = \text{sigmoid}(x)(1 - \text{sigmoid}(x))
$$

sigmoid å‡½æ•°åŠå…¶å¯¼æ•°æ›²çº¿å¦‚ä¸‹æ‰€ç¤º:

![sigmoid å‡½æ•°åŠå…¶å¯¼æ•°å›¾åƒ](./images/activation_function/sigmoid_and_gradient_curve.png)

æ³¨æ„ï¼Œå½“è¾“å…¥ä¸º 0 æ—¶ï¼Œsigmoid å‡½æ•°çš„å¯¼æ•°è¾¾åˆ°æœ€å¤§å€¼ 0.25; è€Œè¾“å…¥åœ¨ä»»ä¸€æ–¹å‘ä¸Šè¶Šè¿œç¦» 0 ç‚¹æ—¶ï¼Œå¯¼æ•°è¶Šæ¥è¿‘ `0`ã€‚

ç›®å‰ `sigmoid` å‡½æ•°åœ¨éšè—å±‚ä¸­å·²ç»è¾ƒå°‘ä½¿ç”¨ï¼ŒåŸå› æ˜¯ `sigmoid` çš„è½¯é¥±å’Œæ€§ï¼Œä½¿å¾—æ·±åº¦ç¥ç»ç½‘ç»œåœ¨è¿‡å»çš„äºŒä¸‰åå¹´é‡Œä¸€ç›´éš¾ä»¥æœ‰æ•ˆçš„è®­ç»ƒï¼Œå¦‚ä»Šå…¶è¢«æ›´ç®€å•ã€æ›´å®¹æ˜“è®­ç»ƒçš„ `ReLU` ç­‰æ¿€æ´»å‡½æ•°æ‰€æ›¿ä»£ã€‚

å½“æˆ‘ä»¬æƒ³è¦è¾“å‡ºäºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»ã€å¤šæ ‡ç­¾é—®é¢˜çš„æ¦‚ç‡æ—¶ï¼Œ`sigmoid` **å¯ç”¨ä½œæ¨¡å‹æœ€åä¸€å±‚çš„æ¿€æ´»å‡½æ•°**ã€‚ä¸‹è¡¨æ€»ç»“äº†å¸¸è§é—®é¢˜ç±»å‹çš„æœ€åä¸€å±‚æ¿€æ´»å’ŒæŸå¤±å‡½æ•°ã€‚

|é—®é¢˜ç±»å‹|æœ€åä¸€å±‚æ¿€æ´»|æŸå¤±å‡½æ•°|
|-------|----------|-------|
|äºŒåˆ†ç±»é—®é¢˜ï¼ˆbinaryï¼‰|`sigmoid`|`sigmoid + nn.BCELoss`(): æ¨¡å‹æœ€åä¸€å±‚éœ€è¦ç»è¿‡ ` torch.sigmoid` å‡½æ•°|
|å¤šåˆ†ç±»ã€å•æ ‡ç­¾é—®é¢˜ï¼ˆMulticlassï¼‰|`softmax`|`nn.CrossEntropyLoss()`: æ— éœ€æ‰‹åŠ¨åš `softmax`|
|å¤šåˆ†ç±»ã€å¤šæ ‡ç­¾é—®é¢˜ï¼ˆMultilabelï¼‰|`sigmoid`|`sigmoid + nn.BCELoss()`: æ¨¡å‹æœ€åä¸€å±‚éœ€è¦ç»è¿‡ `sigmoid` å‡½æ•°|

> `nn.BCEWithLogitsLoss()` å‡½æ•°ç­‰æ•ˆäº `sigmoid + nn.BCELoss`ã€‚

### Tanh å‡½æ•°

`Tanh`ï¼ˆåŒæ›²æ­£åˆ‡ï¼‰å‡½æ•°ä¹Ÿæ˜¯ä¸€ç§ Sigmoid å‹å‡½æ•°ï¼Œå¯ä»¥çœ‹ä½œæ”¾å¤§å¹¶å¹³ç§»çš„ `Sigmoid` å‡½æ•°ï¼Œå…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š

$$
\text{tanh}(x) = 2\sigma(2x) - 1 = \frac{2}{1 + e^{-2x}} - 1
$$

Sigmoid å‡½æ•°å’Œ Tanh å‡½æ•°æ›²çº¿å¦‚ä¸‹å›¾æ‰€ç¤º:

![Logisticå‡½æ•°å’ŒTanhå‡½æ•°](./images/activation_function/sigmoidå‡½æ•°å’Œtanhå‡½æ•°.png)

å¦å¤–ä¸€ç§  Logistic å‡½æ•°å’Œ Tanh å‡½æ•°çš„å½¢çŠ¶å¯¹æ¯”å›¾:

![Logistic å‡½æ•°å’Œ Tanh å‡½æ•°çš„å½¢çŠ¶](./images/activation_function/Logisticå‡½æ•°å’ŒTanhå‡½æ•°2.png)
> å›¾ç‰‡æ¥æº: ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹å›¾4.2ã€‚

`Tanh` å‡½æ•°åŠå…¶å¯¼æ•°æ›²çº¿å¦‚ä¸‹æ‰€ç¤º:

![tanh_and_gradient](./images/activation_function/tanh_and_gradient.png)

ç»“åˆå‰é¢çš„ `Sigmoid` å‡½æ•°åŠå…¶å¯¼æ•°æ›²çº¿è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œå¯ä»¥çœ‹å‡º `Sigmoid` å’Œ `Tanh` å‡½æ•°åœ¨è¾“å…¥å¾ˆå¤§æˆ–æ˜¯å¾ˆå°çš„æ—¶å€™ï¼Œ**è¾“å‡ºéƒ½å‡ ä¹å¹³æ»‘ä¸”æ¢¯åº¦å¾ˆå°è¶‹è¿‘äº 0**ï¼Œä¸åˆ©äºæƒé‡æ›´æ–°ï¼›ä¸åŒçš„æ˜¯ `Tanh` å‡½æ•°çš„è¾“å‡ºåŒºé—´æ˜¯åœ¨ `(-1,1)` ä¹‹é—´ï¼Œè€Œä¸”æ•´ä¸ªå‡½æ•°æ˜¯ä»¥ 0 ä¸ºä¸­å¿ƒçš„ï¼Œè¿™ä¸ªç‰¹ç‚¹æ¯” `Sigmoid` çš„å¥½ã€‚

**ä¸¤ç§æ¿€æ´»å‡½æ•°å®ç°å’Œå¯è§†åŒ–ä»£ç **å¦‚ä¸‹æ‰€ç¤º:

```python
# example plot for the sigmoid activation function
from math import exp
from matplotlib import pyplot
import matplotlib.pyplot as plt

# sigmoid activation function
def sigmoid(x):
    """1.0 / (1.0 + exp(-x))
    """
    return 1.0 / (1.0 + exp(-x))

def tanh(x):
    """2 * sigmoid(2*x) - 1
    (e^x â€“ e^-x) / (e^x + e^-x)
    """
    # return (exp(x) - exp(-x)) / (exp(x) + exp(-x))
    return 2 * sigmoid(2*x) - 1

def relu(x):
    return max(0, x)

def gradient_relu(x):
    if x < 0:
        return 0
    else:
        return 1

def gradient_sigmoid(x):
    """sigmoid(x)(1âˆ’sigmoid(x))
    """
    a = sigmoid(x)
    b = 1 - a
    return a*b

# 1, define input data
inputs = [x for x in range(-10, 11)]

# 2, calculate outputs
outputs = [sigmoid(x) for x in inputs]
outputs2 = [tanh(x) for x in inputs]

# 3, plot sigmoid and tanh function curve
plt.figure(dpi=100) # dpi è®¾ç½®
plt.style.use('ggplot') # ä¸»é¢˜è®¾ç½®

plt.plot(inputs, outputs, label='sigmoid')
plt.plot(inputs, outputs2, label='tanh')

plt.xlabel("x") # è®¾ç½® x è½´æ ‡ç­¾
plt.ylabel("y")
plt.title('sigmoid and tanh') # æŠ˜çº¿å›¾æ ‡é¢˜
plt.legend()
plt.show()
```

Logistic å‡½æ•°å’Œ Tanh å‡½æ•°éƒ½æ˜¯ Sigmoid å‹å‡½æ•°ï¼Œå…·æœ‰é¥±å’Œæ€§ï¼Œä½†æ˜¯**è®¡ç®—å¼€é”€è¾ƒå¤§**ã€‚å› ä¸ºè¿™ä¸¤ä¸ªå‡½æ•°éƒ½æ˜¯åœ¨ä¸­é—´(0 é™„è¿‘)è¿‘ä¼¼çº¿æ€§ï¼Œä¸¤ç«¯é¥±å’Œã€‚å› æ­¤ï¼Œè¿™ä¸¤ä¸ªå‡½æ•°å¯ä»¥é€šè¿‡åˆ†æ®µå‡½æ•°æ¥è¿‘ä¼¼ã€‚

## ReLU å‡½æ•°åŠå…¶å˜ä½“
### ReLU å‡½æ•°

`ReLU`(Rectified Linear Unitï¼Œä¿®æ­£çº¿æ€§å•å…ƒ)ï¼Œæ˜¯ç›®å‰æ·±åº¦ç¥ç»ç½‘ç»œä¸­**æœ€ç»å¸¸ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°**ï¼Œå®ƒä¿ç•™äº†ç±»ä¼¼ step é‚£æ ·çš„ç”Ÿç‰©å­¦ç¥ç»å…ƒæœºåˆ¶: è¾“å…¥è¶…è¿‡é˜ˆå€¼æ‰ä¼šæ¿€å‘ã€‚å…¬å¼å¦‚ä¸‹æ‰€ç¤º:

$$
ReLU(x) = max\{0,x\} = \left\{\begin{matrix}
x & x\geqslant 0 \\ 
0 & x< 0
\end{matrix}\right.
$$

ä»¥ä¸Šå…¬å¼é€šä¿—ç†è§£å°±æ˜¯ï¼Œ`ReLU` å‡½æ•°ä»…ä¿ç•™æ­£å…ƒç´ å¹¶ä¸¢å¼ƒæ‰€æœ‰è´Ÿå…ƒç´ ã€‚æ³¨æ„: è™½ç„¶åœ¨ `0` ç‚¹ä¸èƒ½æ±‚å¯¼ï¼Œä½†æ˜¯å¹¶ä¸å½±å“å…¶åœ¨ä»¥æ¢¯åº¦ä¸ºä¸»çš„åå‘ä¼ æ’­ç®—æ³•ä¸­å‘æŒ¥æœ‰æ•ˆä½œç”¨ã€‚

1ï¼Œ**ä¼˜ç‚¹**: 

- `ReLU` æ¿€æ´»å‡½æ•°**è®¡ç®—ç®€å•**ï¼›
- å…·æœ‰**å¾ˆå¥½çš„ç¨€ç–æ€§**ï¼Œå¤§çº¦ 50% çš„ç¥ç»å…ƒä¼šå¤„äºæ¿€æ´»çŠ¶æ€ã€‚
- å‡½æ•°åœ¨ $x > 0$ æ—¶å¯¼æ•°ä¸º 1 çš„æ€§è´¨ï¼ˆ**å·¦é¥±å’Œå‡½æ•°**ï¼‰ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒåŠ é€Ÿæ¢¯åº¦ä¸‹é™çš„æ”¶æ•›é€Ÿåº¦ã€‚
> ç›¸å…³ç”Ÿç‰©çŸ¥è¯†: äººè„‘ä¸­åœ¨åŒä¸€æ—¶åˆ»å¤§æ¦‚åªæœ‰ 1% âˆ¼ 4% çš„ç¥ç»å…ƒå¤„äºæ´»è·ƒ çŠ¶æ€ã€‚

2ï¼Œ**ç¼ºç‚¹**: 

- ReLU å‡½æ•°çš„è¾“å‡ºæ˜¯éé›¶ä¸­å¿ƒåŒ–çš„ï¼Œç»™åä¸€å±‚çš„ç¥ç»ç½‘ç»œå¼•å…¥åç½®åç§»ï¼Œä¼š**å½±å“æ¢¯åº¦ä¸‹é™çš„æ•ˆç‡**ã€‚
- ReLU ç¥ç»å…ƒåœ¨è®­ç»ƒæ—¶æ¯”è¾ƒå®¹æ˜“â€œæ­»äº¡â€ã€‚å¦‚æœç¥ç»å…ƒå‚æ•°å€¼åœ¨ä¸€æ¬¡ä¸æ°å½“çš„æ›´æ–°åï¼Œå…¶å€¼å°äº 0ï¼Œé‚£ä¹ˆè¿™ä¸ªç¥ç»å…ƒè‡ªèº«å‚æ•°çš„æ¢¯åº¦æ°¸è¿œéƒ½ä¼šæ˜¯ 0ï¼Œåœ¨ä»¥åçš„è®­ç»ƒè¿‡ç¨‹ä¸­æ°¸è¿œä¸èƒ½è¢«æ¿€æ´»ï¼Œè¿™ç§ç°è±¡è¢«ç§°ä½œâ€œ**æ­»åŒº**â€ã€‚

ReLU æ¿€æ´»å‡½æ•°çš„ä»£ç å®šä¹‰å¦‚ä¸‹:

```python
# pytorch æ¡†æ¶å¯¹åº”å‡½æ•°ï¼š nn.ReLU(inplace=True)
def relu(x):
    return max(0, x)
```
**ReLU æ¿€æ´»å‡½æ•°åŠå…¶å‡½æ•°æ¢¯åº¦å›¾**å¦‚ä¸‹æ‰€ç¤º:

![relu_and_gradient_curve](./images/activation_function/relu_and_gradient_curve.png)

> `ReLU` æ¿€æ´»å‡½æ•°çš„æ›´å¤šå†…å®¹ï¼Œè¯·å‚è€ƒåŸè®ºæ–‡ [Rectified Linear Units Improve Restricted Boltzmann Machines](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)

### Leaky ReLU/PReLU/ELU/Softplus å‡½æ•°

1ï¼Œ`Leaky ReLU` **å‡½æ•°**: ä¸ºäº†ç¼“è§£â€œ**æ­»åŒº**â€ç°è±¡ï¼Œç ”ç©¶è€…å°† ReLU å‡½æ•°ä¸­ $x < 0$ çš„éƒ¨åˆ†è°ƒæ•´ä¸º $\gamma \cdot x$ï¼Œ å…¶ä¸­ $\gamma$ å¸¸è®¾ç½®ä¸º 0.01 æˆ– 0.001 æ•°é‡çº§çš„è¾ƒå°æ­£æ•°ã€‚è¿™ç§æ–°å‹çš„æ¿€æ´»å‡½æ•°è¢«ç§°ä½œ**å¸¦æ³„éœ²çš„ ReLU**ï¼ˆ`Leaky ReLU`ï¼‰ã€‚

$$
\text{Leaky ReLU}(x) = max(0, ğ‘¥) + \gamma\ min(0, x)
= \left\{\begin{matrix}
x & x\geqslant 0 \\ 
\gamma \cdot x & x< 0
\end{matrix}\right.
$$

> è¯¦æƒ…å¯ä»¥å‚è€ƒåŸè®ºæ–‡:[ã€ŠRectifier Nonlinearities Improve Neural Network Acoustic Modelsã€‹](https://www.semanticscholar.org/paper/Rectifier-Nonlinearities-Improve-Neural-Network-Maas/367f2c63a6f6a10b3b64b8729d601e69337ee3cc?p2df)

2ï¼Œ`PReLU` **å‡½æ•°**: ä¸ºäº†è§£å†³ Leaky ReLU ä¸­**è¶…å‚æ•° $\gamma$ ä¸æ˜“è®¾å®š**çš„é—®é¢˜ï¼Œæœ‰ç ”ç©¶è€…æå‡ºäº†å‚æ•°åŒ– ReLU(Parametric ReLUï¼Œ`PReLU`)ã€‚å‚æ•°åŒ– ReLU ç›´æ¥å°† $\gamma$ ä¹Ÿä½œä¸ºä¸€ä¸ªç½‘ç»œä¸­å¯å­¦ä¹ çš„å˜é‡èå…¥æ¨¡å‹çš„æ•´ä½“è®­ç»ƒè¿‡ç¨‹ã€‚å¯¹äºç¬¬ $i$ ä¸ªç¥ç»å…ƒï¼Œ`PReLU` çš„ å®šä¹‰ä¸º:

$$
\text{Leaky ReLU}(x) = max(0, ğ‘¥) + \gamma_{i}\ min(0, x)
= \left\{\begin{matrix}
x & x\geqslant 0 \\ 
\gamma_{i} \cdot x & x< 0
\end{matrix}\right.
$$

> è¯¦æƒ…å¯ä»¥å‚è€ƒåŸè®ºæ–‡:[ã€ŠDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classificationã€‹](https://arxiv.org/abs/1502.01852)

3ï¼Œ`ELU` **å‡½æ•°**: 2016 å¹´ï¼ŒClevert ç­‰äººæå‡ºçš„ `ELU` (Exponential Linear Units) åœ¨å°äºé›¶çš„éƒ¨åˆ†é‡‡ç”¨äº†è´ŸæŒ‡æ•°å½¢å¼ã€‚`ELU`  æœ‰å¾ˆå¤šä¼˜ç‚¹ï¼Œä¸€æ–¹é¢ä½œä¸ºéé¥±å’Œæ¿€æ´»å‡½æ•°ï¼Œå®ƒåœ¨æ‰€æœ‰ç‚¹ä¸Šéƒ½æ˜¯è¿ç»­çš„å’Œå¯å¾®çš„ï¼Œæ‰€ä»¥ä¸ä¼šé‡åˆ°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±çš„é—®é¢˜ï¼›å¦ä¸€æ–¹é¢ï¼Œä¸å…¶ä»–çº¿æ€§éé¥±å’Œæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLU åŠå…¶å˜ä½“ï¼‰ç›¸æ¯”ï¼Œå®ƒæœ‰ç€æ›´å¿«çš„è®­ç»ƒæ—¶é—´å’Œæ›´é«˜çš„å‡†ç¡®æ€§ã€‚

ä½†æ˜¯ï¼Œä¸ ReLU åŠå…¶å˜ä½“ç›¸æ¯”ï¼Œå…¶**æŒ‡æ•°æ“ä½œä¹Ÿå¢åŠ äº†è®¡ç®—é‡**ï¼Œå³æ¨¡å‹æ¨ç†æ—¶ `ELU` çš„æ€§èƒ½ä¼šæ¯” `ReLU` åŠå…¶å˜ä½“æ…¢ã€‚ `ELU` å®šä¹‰å¦‚ä¸‹:

$$
\text{Leaky ReLU}(x) = max(0, ğ‘¥) + min(0, \gamma(exp(x) - 1)
= \left\{\begin{matrix}
x & x\geqslant 0 \\ 
\gamma(exp(x) - 1) & x< 0
\end{matrix}\right.
$$

$\gamma â‰¥ 0$ æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œå†³å®š $x â‰¤ 0$ æ—¶çš„é¥±å’Œæ›²çº¿ï¼Œå¹¶è°ƒæ•´è¾“å‡ºå‡å€¼åœ¨ `0` é™„è¿‘ã€‚

> è¯¦æƒ…å¯ä»¥å‚è€ƒåŸè®ºæ–‡:[ã€ŠFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)ã€‹](https://arxiv.org/abs/1511.07289)

4ï¼Œ`Softplus` **å‡½æ•°**: Softplus å‡½æ•°å…¶å¯¼æ•°åˆšå¥½æ˜¯ Logistic å‡½æ•°.Softplus å‡½æ•°è™½ç„¶ä¹Ÿå…·æœ‰å•ä¾§æŠ‘åˆ¶ã€å®½ å…´å¥‹è¾¹ç•Œçš„ç‰¹æ€§ï¼Œå´æ²¡æœ‰ç¨€ç–æ¿€æ´»æ€§ã€‚`Softplus` å®šä¹‰ä¸º:

$$
\text{Softplus}(x) = log(1 + exp(x))
$$
> å¯¹ `Softplus` æœ‰å…´è¶£çš„å¯ä»¥é˜…è¯»è¿™ç¯‡è®ºæ–‡: [ã€ŠDeep Sparse Rectifier Neural Networksã€‹](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)ã€‚

æ³¨æ„: **ReLU å‡½æ•°å˜ä½“æœ‰å¾ˆå¤šï¼Œä½†æ˜¯å®é™…æ¨¡å‹å½“ä¸­ä½¿ç”¨æœ€å¤šçš„è¿˜æ˜¯ `ReLU` å‡½æ•°æœ¬èº«**ã€‚

ReLUã€Leaky ReLUã€ELU ä»¥åŠ Softplus å‡½æ•°ç¤ºæ„å›¾å¦‚ä¸‹å›¾æ‰€ç¤º:

![relu_more](./images/activation_function/relu_more.png)

## Swish å‡½æ•°

`Swish` å‡½æ•°[Ramachandran et al., 2017] æ˜¯ä¸€ç§è‡ªé—¨æ§(Self-Gated)æ¿€æ´» å‡½æ•°ï¼Œå®šä¹‰ä¸º

$$
\text{swish}(x) = x\sigma(\beta x)
$$

å…¶ä¸­ $\sigma(\cdot)$ ä¸º Logistic å‡½æ•°ï¼Œ$\beta$ ä¸ºå¯å­¦ä¹ çš„å‚æ•°æˆ–ä¸€ä¸ªå›ºå®šè¶…å‚æ•°ã€‚$\sigma(\cdot) \in (0, 1)$ å¯ä»¥çœ‹ä½œä¸€ç§è½¯æ€§çš„é—¨æ§æœºåˆ¶ã€‚å½“ $\sigma(\beta x)$ æ¥è¿‘äº `1` æ—¶ï¼Œé—¨å¤„äºâ€œå¼€â€çŠ¶æ€ï¼Œæ¿€æ´»å‡½æ•°çš„è¾“å‡ºè¿‘ä¼¼äº $x$ æœ¬èº«ï¼›å½“ $\sigma(\beta x)$ æ¥è¿‘äº `0` æ—¶ï¼Œé—¨çš„çŠ¶æ€ä¸ºâ€œå…³â€ï¼Œæ¿€æ´»å‡½æ•°çš„è¾“å‡ºè¿‘ä¼¼äº `0`ã€‚

`Swish` å‡½æ•°ä»£ç å®šä¹‰å¦‚ä¸‹ï¼Œç»“åˆå‰é¢çš„ç”»æ›²çº¿ä»£ç ï¼Œå¯å¾— Swish å‡½æ•°çš„ç¤ºä¾‹å›¾ã€‚

```python
# sigmoid activation function
def sigmoid(x):
    """1.0 / (1.0 + exp(-x))
    """
    return 1.0 / (1.0 + exp(-x))

def swish(x, beta = 0):
    """swish(ğ‘¥) = ğ‘¥ğœ(ğ›½ğ‘¥)
    beta æ˜¯éœ€è¦æ‰‹åŠ¨è®¾ç½®çš„å‚æ•°
    """
    return x * sigmoid(beta*x)
```

![Swish å‡½æ•°](./images/activation_function/swish_of_different_beta.png)

**Swish å‡½æ•°å¯ä»¥çœ‹ä½œçº¿æ€§å‡½æ•°å’Œ ReLU å‡½æ•°ä¹‹é—´çš„éçº¿æ€§æ’å€¼å‡½æ•°ï¼Œå…¶ç¨‹åº¦ç”±å‚æ•° $\beta$ æ§åˆ¶**ã€‚
## æ¿€æ´»å‡½æ•°æ€»ç»“

å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°åŒ…æ‹¬ `ReLU` å‡½æ•°ã€`sigmoid` å‡½æ•°å’Œ `tanh` å‡½æ•°ã€‚ä¸‹è¡¨æ±‡æ€»æ¯”è¾ƒäº†å‡ ä¸ªæ¿€æ´»å‡½æ•°çš„å±æ€§:

![activation_function](./images/activation_function/activation_function_summary.png)

**æ¿€æ´»å‡½æ•°çš„åœ¨çº¿å¯è§†åŒ–**ç§»æ­¥ [Visualising Activation Functions in Neural Networks](https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/)ã€‚

## å‚è€ƒèµ„æ–™

1. [Pytorchåˆ†ç±»é—®é¢˜ä¸­çš„äº¤å‰ç†µæŸå¤±å‡½æ•°ä½¿ç”¨](https://www.cnblogs.com/hmlovetech/p/14515622.html)
2. ã€Šè§£æå·ç§¯ç¥ç»ç½‘ç»œ-ç¬¬8ç« ã€‹
3. ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ -ç¬¬4ç« ã€‹
4. [How to Choose an Activation Function for Deep Learning](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)
5. [æ·±åº¦å­¦ä¹ ä¸­çš„æ¿€æ´»å‡½æ•°æ±‡æ€»](http://spytensor.com/index.php/archives/23/)
6. [Visualising Activation Functions in Neural Networks](https://dashee87.github.io/deep%20learning/visualising-activation-functions-in-neural-networks/)